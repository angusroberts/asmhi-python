{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting to a LLM hosted at a Hugging Face endpoint"
      ],
      "metadata": {
        "id": "1itC_4Z1tdtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For documentation see https://python.langchain.com/docs/integrations/llms/huggingface_pipelines\n"
      ],
      "metadata": {
        "id": "LyQ3t2iXVou7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be using LangChain\n",
        "!pip install --quiet langchain langchain_community\n",
        "import langchain_community\n",
        "from langchain_community.llms import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "I_UanP0nUdV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Huggingface endpoints\n",
        "\n",
        "From https://python.langchain.com/docs/integrations/llms/huggingface_endpoint\n",
        "\n",
        "Huggingface serverless inference is described here:\n",
        "\n",
        "https://huggingface.co/docs/api-inference/index\n",
        "\n",
        "This explains how to get an API token from your account, and how to construct the model URL. It also gives code for using a model, but we will instead used Langchain to interface to the Huggingface API.\n",
        "\n",
        "You can set up your own dedicated endpoint to which you deploy a model, giving better availability than the public endpoints. There is a cost:\n",
        "\n",
        "https://huggingface.co/inference-endpoints/dedicated\n",
        "\n",
        "UI for starting and stopping and configuring endpoints is here:\n",
        "\n",
        "https://ui.endpoints.huggingface.co/angusroberts/endpoints\n",
        "\n",
        "\n",
        "protected endpoint - seems to need own token\n",
        "public endpoint - still needs a token, but can be any\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTrGwafqt4tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Hugging Face account and API token\n",
        "\n",
        "We will be using a LLM hosted on a Hugging Face endpoint. In order to use this, you will need a Hugging Face account and API token.\n",
        "\n",
        "### Hugging Face account\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   Open https://huggingface.co/\n",
        "1.   Click the \"Sign Up\" button at top right\n",
        "1.   Fill in your email address and a new password and click \"Next\"\n",
        "1.   Enter details on your user profile page. You will need to enter a username and full name, the rest is optional\n",
        "1.   Select the terms and conditions check box\n",
        "1.   Click \"Create Account\"\n",
        "1.   You will be sent a verificaiton email with a link to click, completing account creation\n",
        "\n",
        "### Hugging Face API token\n",
        "\n",
        "1. Log in to Hugging Face https://huggingface.co/\n",
        "1. Click your account icon at top right\n",
        "1. Select the \"Settings\" option towards the bottom of the menu\n",
        "1. From the Settings menu on the left of the window, select \"Access Tokens\"\n",
        "1. Click the \"New token\" button\n",
        "1. In the dialog box, give your token a name, and choose token type of \"Read\", then click \"Generate token\"\n",
        "1. Your token will appear as a string hidden by asterisks\n",
        "1. Copy the token to your clipboard by clicking the copy icon just to its right\n",
        "1. In the next code cell, you will paste it in to a variable and make it available to your local environment\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AM3d6F8IJTKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass(prompt = 'Paste in your Hugging Face API token and press enter:')\n",
        "\n",
        "# We put the token in an environment variable, from where Langchain will access it when needed\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "-st7mi8CyvOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where our endpoint is hosted\n",
        "endpoint_url = \"https://j278zkynwm0b3dky.eu-west-1.aws.endpoints.huggingface.cloud\""
      ],
      "metadata": {
        "id": "eT8dB6qODpsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will create a LangChain LLM from a HuggingFaceEndpoint\n",
        "# at our endpoint URL\n",
        "llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    add_to_git_credential=True,\n",
        "    max_new_tokens=512,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ],
      "metadata": {
        "id": "haa9Lmuo1blM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We should now be able to interact with the endpoint, e.g. by sending a prompt\n",
        "# and getting back a response\n",
        "llm.invoke(\"Why did the chicken cross the road?\")"
      ],
      "metadata": {
        "id": "bGsGg4KoQP9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
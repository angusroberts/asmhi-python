{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bag-of-words.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpbT5yTXIFaT"
      },
      "source": [
        "# Bag of Words - BoW\n",
        "\n",
        "With acknowledgements to Rahul Vasaikar https://github.com/rahulvasaikar\n",
        "\n",
        "This notebook builds a simple bag of words model from first principles. We will then see how you can use the SciKitLearn module to build BoW models, and visualises it along a small number of axes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbSrdIgm1tsR"
      },
      "source": [
        "Firsrt, we will import some useful modules for handling strings and collections, and also SciKit and pandas for our second version of BoW."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ESh_zb_I6gG"
      },
      "source": [
        "# Import some useful modules, to implement a Bag of Words from scratch\n",
        "import string\n",
        "import pprint\n",
        "from collections import Counter\n",
        "\n",
        "# Modules we will use to implement another version of Bag of Words with SciKitLearn\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM3Wy36C1rM9"
      },
      "source": [
        "Let's make some simple documents, in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDPxrbJ5IQF_"
      },
      "source": [
        "documents = ['Klonopin 0.25 mg po every evening, Fluconazole 200 mg po daily, Synthroid 125 mcg po every day',\n",
        "             'She will not consider switching to clozapine',\n",
        "             'lovastatin 40 mg one half tab po daily, multivitamin daily, metformin 500 mg one tab po twice a day',\n",
        "             'Aspirin 81 mg po once daily, Zoloft 25 mg po once daily, Calcium with vitamin D two tablets po once daily']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFYwj4YKZZw"
      },
      "source": [
        "Let's \"normalise\" our documents, to remove punctuation and case differences. We could do more here - what NLP techniques might you apply to iron out differences between similar words?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDN7LUW4I0fU"
      },
      "source": [
        "normalised_documents = []\n",
        "for i in documents:\n",
        "    no_punctuation = ''.join(c for c in i if c not in string.punctuation)\n",
        "    normalised_documents.append(no_punctuation.lower())\n",
        "    \n",
        "for i in normalised_documents:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ0G2cipMJfC"
      },
      "source": [
        "Now let's split them up in to tokens, by splitting at whitespace. We could use a tokeniser for this, e.g. from nltk. Why might this be better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWDc2C7TMRVW"
      },
      "source": [
        "tokenised_documents = []\n",
        "for i in normalised_documents:\n",
        "    tokenised_documents.append(i.split(' '))\n",
        "\n",
        "for i in tokenised_documents:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps4vk10rMxAa"
      },
      "source": [
        "Let's  find the frequency of each unique token in our documents, i.e. the Bag of Words - BoW."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7x6ICnINGEr"
      },
      "source": [
        "frequency_list = []\n",
        "import pprint\n",
        "from collections import Counter\n",
        "\n",
        "for i in tokenised_documents:\n",
        "    frequency_list.append(Counter(i))\n",
        "\n",
        "pp = pprint.PrettyPrinter(width=200)\n",
        "pp.pprint(frequency_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdLrH4iCQqZC"
      },
      "source": [
        "Now let's do the same with SciKitLearn, using the CountVectorizer class\n",
        ". We define a token pattern that excludes numbers, and we also remove english stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFrooygARcsZ"
      },
      "source": [
        "count_vector = CountVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b', stop_words = 'english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yhT9wkgRxPP"
      },
      "source": [
        "Now let's run our vectorizer, to make the bag of words. We will print our token features. Note how all punctuation has been removed by default. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKUlfKf1UDtV"
      },
      "source": [
        "count_vector.fit(documents)\n",
        "count_vector.get_feature_names_out()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S18RaPkVs7q"
      },
      "source": [
        "Let's transform our  documents in to count vectors and take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equrYFb_VtSH"
      },
      "source": [
        "doc_array = count_vector.transform(documents).toarray()\n",
        "print(doc_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBYfGwX9V7vc"
      },
      "source": [
        "And looking at how this encodes each document against the word dimensions: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N6WD7ulV8Cs"
      },
      "source": [
        "frequency_matrix = pd.DataFrame(doc_array,index=documents,columns=count_vector.get_feature_names_out())\n",
        "frequency_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NqxYyHdWZX4"
      },
      "source": [
        "We can spot the difference between our documents. To imagine what it would look like if we plotted these in a multidimensional space, with one dimension for each word in our vocabulary, let's restrict our vocabulary to just three of the words for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp5XuPFPWZyr"
      },
      "source": [
        "count_vector = CountVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b', stop_words = 'english', vocabulary=['daily','mg','po'])\n",
        "count_vector.fit(documents)\n",
        "doc_array = count_vector.transform(documents).toarray()\n",
        "frequency_matrix = pd.DataFrame(doc_array,index=documents,columns=count_vector.get_feature_names_out())\n",
        "frequency_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3QidNK8CyfY"
      },
      "source": [
        "Our plotting module needs one array for  each of our three dimensions, instead of one for each document:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr2FoZP0CzdO"
      },
      "source": [
        "rotated = list(zip(*doc_array[::-1]))\n",
        "print(rotated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5nPWRh1Dkss"
      },
      "source": [
        "And now the plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqzBxex3DlfM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "z,x,y = rotated\n",
        "ax.scatter(list(x), list(y), list(z), zdir='z', c= 'red')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}